# important packages 

import numpy as np #main package for scientific computing with Python.
import h5py  # read .h5 files
import matplotlib.pyplot as plt  # a library to plot graphs in Python.
from testCases_v4a import *
from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward

%matplotlib inline
plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'

%load_ext autoreload
%autoreload 2

np.random.seed(1)

#-----------------------------#

# load data 

train_x_orig, train_y, test_x_orig, test_y, classes = load_data()

#-----------------------------#

# Explore your dataset 

m_train = train_x_orig.shape[0]
num_px = train_x_orig.shape[1]
m_test = test_x_orig.shape[0]

print ("Number of training examples: " + str(m_train))
print ("Number of testing examples: " + str(m_test))
print ("Each image is of size: (" + str(num_px) + ", " + str(num_px) + ", 3)")
print ("train_x_orig shape: " + str(train_x_orig.shape))
print ("train_y shape: " + str(train_y.shape))
print ("test_x_orig shape: " + str(test_x_orig.shape))
print ("test_y shape: " + str(test_y.shape))

#-----------------------------#

# Standardize data to have feature values between 0 and 1.

train_x = train_x_flatten/255.
test_x = test_x_flatten/255.

print ("train_x's shape: " + str(train_x.shape))
print ("test_x's shape: " + str(test_x.shape))

#-----------------------------#

# L -layer model algorithm

layers_dims = [12288, 20, 7, 5, 1] - 4 layer model

def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False, lambd = 0):

# parameter initialization

    parameters = {}
    L = len(layers_dims)            # number of layers in the network
    for l in range(1, L):
        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * np.sqrt(2/layers_dims[l-1])       ## He initialisation works well with ReLu activation function
        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))

        
        assert(parameters['W' + str(l)].shape == (layers_dims[l], layers_dims[l-1]))
        assert(parameters['b' + str(l)].shape == (layers_dims[l], 1))

# Loop (gradient descent)
    for i in range(0, num_iterations):
	# Forward propagation: [LINEAR -> RELU]
	 A = X
    	 for l in range(1, L+1):
        	A_prev = A 
		Z[l] = np.dot(parameters['W' + str(l)],A_prev) + parameters['b' + str(l)]
       		A[l] = relu(Z[l])

   	 ## Implement LINEAR -> SIGMOID. Add "cache" to the "caches" list.
         ##Z[L] = np.dot(parameters['W' + str(l)],A_prev) + parameters['b' + str(l)]
       	 ##A[L] = sigmoid(Z[L])

	# Compute cost 
	m = X.shape[1]
	 logprobs = np.multiply(Y,np.log(A[L])) + np.multiply((1-Y),np.log(1-A[L]))
    	 costNR = (-1/m)*np.sum(logprobs)
	## cost with regularisation
	sum =0;
	for l in range(L):
		sum = sum + np.sum(np.square(W[l+1]))
	costR = (lambd/(2*m))*sum
	cost = cosyNR + costR
	cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).
   	assert(cost.shape == ())

 
	# Backward propagation.
	 dAL = - (np.divide(Y, A]L]) - np.divide(1 - Y, 1 - A[L])) 
	 for l in reversed(range(L)): 
		dZ[l+1] = relu_backward(dA, activation_cache) ##(means dZ[l+1] = dA[l+1]*g'(Z[l+1]))
		dW[l+1] = (1/m)*np.dot(dZ[l+1],A[l].T) + (lambd/m)*W[l+1]
   		db[l+1] = (1/m)*np.sum(dZ[l+1], axis=1, keepdims=True)
		dA[l]  = np.dot(W[l+1].T,dZ[l+1])

    	# Update parameters
	for l in range(L):
       		 parameters["W" + str(l+1)] = parameters["W" + str(l+1)] - learning_rate*grads["dW" + str(l+1)]
         	 parameters["b" + str(l+1)] = parameters["b" + str(l+1)] - learning_rate*grads["db" + str(l+1)]

return parameters

parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True, lambd = 0)
pred_train = predict(train_x, train_y, parameters)
pred_test = predict(test_x, test_y, parameters)